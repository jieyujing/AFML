# 29. Trafalis, T. and H. Ince (2000): “Support vector machine for regression

29. Trafalis, T. and H. Ince (2000): “Support vector machine for regression
and applications to financial forecasting.” Neural Networks , Vol. 6, No. 1,
pp. 348–353.
30. Trippi, R. and D. DeSieno (1992): “Trading equity index futures with a
neural network.” Journal of Portfolio Management , Vol. 19, No. 1, pp.
27–33.
31. Tsai, C. and S. Wang (2009): “Stock price forecasting by hybrid machine
learning techniques.” Proceedings of the International Multi-Conference
of Engineers and Computer Scientists , Vol. 1, No. 1, pp. 755–760.
32. Tsai, C., Y. Lin, D. Yen, and Y. Chen (2011): “Predicting stock returns by
classifier ensembles.” Applied Soft Computing , Vol. 11, No. 2, pp. 2452–
2459.
33. Wang, J. and S. Chan (2006): “Stock market trading rule discovery using
two-layer bias decision tree.” Expert Systems with Applications , Vol. 30,
No. 4, pp. 605–611.
34. Wang, Q., J. Li, Q. Qin, and S. Ge (2011): “Linear, adaptive and nonlinear
trading models for Singapore Stock Market with random for

---

1    You are probably aware of at least one large hedge fund that monitors the
emotional state of their research analysts on a daily basis.
CHAPTER 4
Sample Weights
4.1 Motivation
Chapter 3 presented several new methods for labeling financial observations.
We introduced two novel concepts, the triple-barrier method and meta-labeling,
and explained how they are useful in financial applications, including
quantamental investment strategies. In this chapter you will learn how to use
sample weights to address another problem ubiquitous in financial applications,
namely that observations are not generated by independent and identically
distributed (IID) processes. Most of the ML literature is based on the IID
assumption, and one reason many ML applications fail in finance is because
those assumptions are unrealistic in the case of financial time series.
4.2 Overlapping Outcomes
In Chapter 3 we assigned a label y i to an observed feature X i , where y i was a
function of price bars that occu

### Code Examples

```unknown
useful in financial applications, including
```

```unknown
use every feature outcome is
```

```unknown
used to determine the outcome. On one hand, if we wished to
```

---

dependent labeling technique, like the triple-barrier method, the sampling
frequency would be subordinated to the first barrier's touch. No matter what
you do, restricting the outcome's horizon to eliminate overlaps is a terrible
solution. We must allow t i , 1 > t i + 1, 0 , which brings us back to the problem of
overlapping outcomes described earlier.
This situation is characteristic of financial applications. Most non-financial ML
researchers can assume that observations are drawn from IID processes. For
example, you can obtain blood samples from a large number of patients, and
measure their cholesterol. Of course, various underlying common factors will
shift the mean and standard deviation of the cholesterol distribution, but the
samples are still independent: There is one observation per subject. Suppose
you take those blood samples, and someone in your laboratory spills blood
from each tube into the following nine tubes to their right. That is, tube 10
contains blood for patient 

---

compute the number of labels concurrent at t , 
 . Snippet 4.1
illustrates an implementation of this logic.
SNIPPET 4.1 ESTIMATING THE UNIQUENESS OF A LABEL
4.4 Average Uniqueness of a Label
In this section we are going to estimate a label's uniqueness (non-overlap) as its
average uniqueness over its lifespan. First, the uniqueness of a label i at time t
is u t , i = 1 t , i c − 1 
t . Second, the average uniqueness of label i is the average u t
, i over the label's lifespan, 
 . This average
uniqueness can also be interpreted as the reciprocal of the harmonic average of
c t over the event's lifespan. Figure 4.1 plots the histogram of uniqueness values
derived from an object t1 . Snippet 4.2 implements this calculation.

---

Figure 4.1 Histogram of uniqueness values
SNIPPET 4.2 ESTIMATING THE AVERAGE UNIQUENESS OF A
LABEL

---

Note that we are making use again of the function mpPandasObj , which speeds
up calculations via multiprocessing (see Chapter 20). Computing the average
uniqueness associated with label i ,  , requires information that is not
available until a future time, events['t1'] . This is not a problem, because
 are used on the training set in combination with label information,
and not on the testing set. These 
 are not used for forecasting the
label, hence there is no information leakage. This procedure allows us to assign
a uniqueness score between 0 and 1 for each observed feature, in terms of non-
overlapping outcomes.
4.5 Bagging Classifiers and Uniqueness
The probability of not selecting a particular item i after I draws with
replacement on a set of I items is (1 − I − 1 ) I . As the sample size grows, that
probability converges to the asymptotic value 
 . That
means that the number of unique observations drawn is expected to be
 .
Suppose that the maximum number of non-overlapping outco

### Code Examples

```unknown
use again of the function mpPandasObj , which speeds
```

```unknown
requires information that is not
```

```unknown
used on the training set in combination with label information,
```

---

That means that the number of unique observations drawn is expected to be
 . The implication is that incorrectly assuming IID draws
leads to oversampling.
When sampling with replacement (bootstrap) on observations with
 , it becomes increasingly likely that in-bag observations will be
(1) redundant to each other, and (2) very similar to out-of-bag observations.
Redundancy of draws makes the bootstrap inefficient (see Chapter 6). For
example, in the case of a random forest, all trees in the forest will essentially be
very similar copies of a single overfit decision tree. And because the random
sampling makes out-of-bag examples very similar to the in-bag ones, out-of-
bag accuracy will be grossly inflated. We will address this second issue in
Chapter 7, when we study cross-validation under non-IID observations. For the
moment, let us concentrate on the first issue, namely bagging under
observations where 
 .
A first solution is to drop overlapping outcomes before performing the
bootstra

### Code Examples

```unknown
max_samples=out['tW'].mean()
```

```unknown
use overlaps are not perfect, dropping an observation just
```

```unknown
use there is a partial overlap will result in an extreme loss of information. I
```

---

alternative method that addresses directly the problem of overlapping
outcomes.
First, an observation X i is drawn from a uniform distribution, i ∼ U [1, I ], that
is, the probability of drawing any particular value i is originally δ (1) 
i = I − 1 .
For the second draw, we wish to reduce the probability of drawing an
observation X j with a highly overlapping outcome. Remember, a bootstrap
allows sampling with repetition, so it is still possible to draw X i again, but we
wish to reduce its likelihood, since there is an overlap (in fact, a perfect
overlap) between X i and itself. Let us denote as φ the sequence of draws so far,
which may include repetitions. Until now, we know that φ (1) = { i }. The
uniqueness of j at time t is 
 , as that is the
uniqueness that results from adding alternative j ’s to the existing sequence of
draws φ (1) . The average uniqueness of j is the average u (2) 
t , j over j ’s lifespan,
 . We can now make a second draw based on the
updated probabilities {δ (

### Code Examples

```unknown
include repetitions. Until now, we know that φ (1) = { i }. The
```

```unknown
used multiple times in Chapter 3.
```

---

features are observed, and a values array containing the time at which the label
is determined. The output of this function is a binary matrix indicating what
(price) bars influence the label for each observation.
SNIPPET 4.3 BUILD AN INDICATOR MATRIX
Snippet 4.4 returns the average uniqueness of each observed feature. The input
is the indicator matrix built by getIndMatrix .
SNIPPET 4.4 COMPUTE AVERAGE UNIQUENESS
Snippet 4.5 gives us the index of the features sampled by sequential bootstrap.
The inputs are the indicator matrix ( indM ) and an optional sample length (
sLength ), with a default value of as many draws as rows in indM .
SNIPPET 4.5 RETURN SAMPLE FROM SEQUENTIAL
BOOTSTRAP

### Code Examples

```unknown
getIndMatrix
```

---

4.5.3 A Numerical Example
Consider a set of labels { y i } i = 1, 2, 3 , where label y 1 is a function of return r 0,
3 , label y 2 is a function of return r 2, 4 and label y 3 is a function of return r 4, 6 .
The outcomes’ overlaps are characterized by this indicator matrix {1 t , i },
The procedure starts with φ (0) = ∅, and a uniform distribution of probability,
 , ∀ i = 1, 2, 3 . Suppose that we randomly draw a number from {1, 2, 3},
and 2 is selected. Before we make a second draw on {1, 2, 3} (remember, a
bootstrap samples with repetition), we need to adjust the probabilities. The set
of observations drawn so far is φ (1) = {2}. The average uniqueness for the first
feature is 
 , and for the second feature is
 . The probabilities for the second draw are
 . Two points are worth mentioning: (1) The lowest

---

